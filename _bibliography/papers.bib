@inproceedings{higuera2024sparsh,
    abbr = {CoRL},
    pdf = {sparsh.pdf},
    preview = {sparsh.gif},
    selected = {true},
    html = {https://sparsh-ssl.github.io/},
    title={Sparsh: Self-supervised touch representations for vision-based tactile sensing},
    abstract={
        In this work, we introduce general purpose touch representations for the increasingly accessible 
        class of vision-based tactile sensors. Such sensors have led to many recent advances in robot
        manipulation as they markedly complement vision, yet solutions today often rely on task 
        and sensor specific handcrafted perception models. Collecting real data at scale with task 
        centric ground truth labels, like contact forces and slip, is a challenge further compounded 
        by sensors of various form factor differing in aspects like lighting and gel markings. 
        To tackle this we turn to self-supervised learning (SSL) that has demonstrated remarkable 
        performance in computer vision. We present Sparsh, a family of SSL models 
        that can support various vision-based tactile sensors, alleviating the need for custom labels 
        through pre-training on 460k+ tactile images with masking and self-distillation in pixel and latent spaces. 
        We also build TacBench, to facilitate standardized benchmarking across sensors and models, 
        comprising of six tasks ranging from comprehending tactile properties to enabling physical 
        perception and manipulation planning. In evaluations, we find that SSL pre-training for 
        touch representation outperforms task and sensor-specific end-to-end training by 95.1%
        on average over TacBench, and Sparsh (DINO)
        Sparsh (IJEPA) are the most competitive, 
        indicating the merits of learning in latent space for tactile images. 
    },
    author={Carolina Higuera* and Akash Sharma* and Chaithanya Krishna Bodduluri and Taosha Fan and Mrinal Kalakrishnan and Michael Kaess and Byron Boots and Mike Lambeta and Tingfan Wu and Mustafa Mukadam},
    booktitle={8th Annual Conference on Robot Learning},
    year={2024},
    url={https://openreview.net/forum?id=xYJn2e1uu8}
}
@inproceedings{Chang_2023_ICCV,
    abbr = {ICCV},
    pdf = {Chang23iccv.pdf},
    preview = {nerf_lidar.png},
    author    = {Chang, MingFang and Sharma, Akash and Kaess, Michael and Lucey, Simon},
    title     = {Neural Radiance Field with LiDAR maps},
    abstract  = {We address outdoor Neural Radiance Fields (NeRF) with LiDAR maps. 
                 Existing NeRF methods usually require specially collected hypersampled source views 
                 and do not perform well with the open source camera-LiDAR datasets - significantly limiting 
                 the approach's practical utility. In this paper, we demonstrate an approach that allows for 
                 these datasets to be utilized for high quality neural renderings. Our design leverages 
                 1) LiDAR sensors for strong 3D geometry priors that significantly improve the ray sampling locality, 
                 and 2) Conditional Adversarial Networks (cGANs) to recover image details since aggregating 
                 embeddings from imperfect LiDAR maps causes artifacts in the synthesized images. 
                 Our experiments show that while NeRF baselines produce either noisy or blurry results on Argoverse 2, 
                 the images synthesized by our system not only outperform baselines in image quality metrics under both 
                 clean and noisy conditions, but also obtain closer Detectron2 results to the ground truth images. Furthermore, 
                 to show the substantial applicability of our system, we demonstrate that our system can be used in 
                 data augmentation for training a pose regression network and multi-season view synthesis. 
                 Our dataset and code will be released.},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision, ICCV},
    month     = {October},
    year      = {2023},
    pages     = {17914-17923}
}

@inproceedings{Xu22iros,
    abbr = {IROS},
    pdf = {Xu22iros.pdf},
    author = {Ruoyang Xu and Wei Dong and Akash Sharma and Michael Kaess},
    title = {Learned Depth Estimation of {3D} Imaging Radar for Indoor Mapping},
    abstract = {3D imaging radar offers robust perception capability through
                visually demanding environments due to the unique penetrative and
                reflective properties of millimeter waves (mmWave). Current
                approaches for 3D perception with imaging radar require knowledge
                of environment geometry, accumulation of data from multiple
                frames for perception, or access to between-frame motion. Imaging
                radar presents an additional difficulty due to the complexity of
                its data representation. To address these issues, and make
                imaging radar easier to use for downstream robotics tasks, we
                propose a learning-based method that regresses radar measurements
                into cylindrical depth maps using LiDAR supervision. Due to the
                limitation of the regression formulation, directions where the
                radar beam could not reach will still generate a valid depth. To
                address this issue, our method additionally learns a 3D filter to
                remove those pixels. Experiments show that our system generates
                visually accurate depth estimation. Furthermore, we confirm the
                overall ability to generalize in the indoor scene using the
                estimated depth for probabilistic occupancy mapping with ground
                truth trajectory.},
    booktitle = {Proc. IEEE/RSJ Intl. Conf. on Intelligent Robots and Systems,
                 IROS},
    address = {Kyoto, Japan},
    month = oct,
    year = {2022},
    pages={13260-13267},
}

@inproceedings{Sharma21icra,
    abbr = {ICRA},
    selected = {true},
    preview = {object_slam.gif},
    pdf = {Sharma21icra.pdf},
    code = {https://github.com/rpl-cmu/object-slam},
    teaser = {object_slam.gif},
    author = {Akash Sharma and Wei Dong and Michael Kaess},
    title = {Compositional and Scalable Object {SLAM}},
    abstract = {We present a fast, scalable, and accurate Simultaneous
                Localization and Mapping (SLAM) system that represents indoor
                scenes as a graph of objects. Leveraging the observation that
                artificial environments are structured and occupied by
                recognizable objects, we show that a compositional and scalable
                object mapping formulation is amenable to a robust SLAM solution
                for drift-free large-scale indoor reconstruction. To achieve this
                , we propose a novel semantically assisted data association
                strategy that results in unambiguous persistent object landmarks
                and a 2.5D compositional rendering method that enables reliable
                frame-to-model RGB-D tracking. Consequently, we deliver an
                optimized online implementation that can run at near frame rate
                with a single graphics card, and provide a comprehensive
                evaluation against state-of-the-art baselines. An open-source
                implementation will be provided at
                https://github.com/rpl-cmu/object-slam.},
    booktitle = {Proc. IEEE Intl. Conf. on Robotics and Automation, ICRA},
    address = {Xi'an, China},
    month = may,
    year = {2021},
}

@thesis{Akash-Sharma-2021-127395,
    abbr = {M.S.},
    author = {Akash Sharma},
    pdf = {msr_thesis.pdf},
    title = {Incorporating Semantic Structure in SLAM},
    year = {2021},
    month = {May},
    school = {Carnegie Mellon University},
    address = {Pittsburgh, PA},
    number = {CMU-RI-TR-21-18},
    keywords = {Semantic SLAM, SLAM, Instance Segmentation, Dense reconstruction
                },
}