---
---

@inproceedings{Xu22iros,
    abbr = {IROS},
    pdf = {Xu22iros.pdf},
    author = {R. Xu and W. Dong and A. Sharma and M. Kaess},
    title = {Learned Depth Estimation of {3D} Imaging Radar for Indoor Mapping},
    abstract = {3D imaging radar offers robust perception capability through
                visually demanding environments due to the unique penetrative and
                reflective properties of millimeter waves (mmWave). Current
                approaches for 3D perception with imaging radar require knowledge
                of environment geometry, accumulation of data from multiple
                frames for perception, or access to between-frame motion. Imaging
                radar presents an additional difficulty due to the complexity of
                its data representation. To address these issues, and make
                imaging radar easier to use for downstream robotics tasks, we
                propose a learning-based method that regresses radar measurements
                into cylindrical depth maps using LiDAR supervision. Due to the
                limitation of the regression formulation, directions where the
                radar beam could not reach will still generate a valid depth. To
                address this issue, our method additionally learns a 3D filter to
                remove those pixels. Experiments show that our system generates
                visually accurate depth estimation. Furthermore, we confirm the
                overall ability to generalize in the indoor scene using the
                estimated depth for probabilistic occupancy mapping with ground
                truth trajectory.},
    booktitle = {Proc. IEEE/RSJ Intl. Conf. on Intelligent Robots and Systems,
                 IROS},
    address = {Kyoto, Japan},
    month = oct,
    year = {2022},
    note = {To appear},
}

@inproceedings{Sharma21icra,
    abbr = {ICRA},
    selected = {true},
    pdf = {Sharma21icra.pdf},
    code = {https://github.com/rpl-cmu/object-slam},
    teaser = {object_slam.gif},
    author = {A. Sharma and W. Dong and M. Kaess},
    title = {Compositional and Scalable Object {SLAM}},
    abstract = {We present a fast, scalable, and accurate Simultaneous
                Localization and Mapping (SLAM) system that represents indoor
                scenes as a graph of objects. Leveraging the observation that
                artificial environments are structured and occupied by
                recognizable objects, we show that a compositional and scalable
                object mapping formulation is amenable to a robust SLAM solution
                for drift-free large-scale indoor reconstruction. To achieve this
                , we propose a novel semantically assisted data association
                strategy that results in unambiguous persistent object landmarks
                and a 2.5D compositional rendering method that enables reliable
                frame-to-model RGB-D tracking. Consequently, we deliver an
                optimized online implementation that can run at near frame rate
                with a single graphics card, and provide a comprehensive
                evaluation against state-of-the-art baselines. An open-source
                implementation will be provided at
                https://github.com/rpl-cmu/object-slam.},
    booktitle = {Proc. IEEE Intl. Conf. on Robotics and Automation, ICRA},
    address = {Xi'an, China},
    month = may,
    year = {2021},
}

@thesis{Akash-Sharma-2021-127395,
    abbr = {M.S.},
    author = {Akash Sharma},
    pdf = {msr_thesis.pdf},
    title = {Incorporating Semantic Structure in SLAM},
    year = {2021},
    month = {May},
    school = {Carnegie Mellon University},
    address = {Pittsburgh, PA},
    number = {CMU-RI-TR-21-18},
    keywords = {Semantic SLAM, SLAM, Instance Segmentation, Dense reconstruction
                },
}

@article{1638022,
    author = {Durrant-Whyte, H. and Bailey, T.},
    journal = {IEEE Robotics Automation Magazine},
    title = {Simultaneous localization and mapping: part I},
    year = {2006},
    volume = {13},
    number = {2},
    pages = {99-110},
    doi = {10.1109/MRA.2006.1638022},
}
