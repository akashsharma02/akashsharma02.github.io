---
---
---
---
@inproceedings{Chang_2023_ICCV,
    abbr = {ICCV},
    pdf = {Chang23iccv.pdf},
    author    = {Chang, MingFang and Sharma, Akash and Kaess, Michael and Lucey, Simon},
    title     = {Neural Radiance Field with LiDAR maps},
    abstract  = {We address outdoor Neural Radiance Fields (NeRF) with LiDAR maps. 
                 Existing NeRF methods usually require specially collected hypersampled source views 
                 and do not perform well with the open source camera-LiDAR datasets - significantly limiting 
                 the approach's practical utility. In this paper, we demonstrate an approach that allows for 
                 these datasets to be utilized for high quality neural renderings. Our design leverages 
                 1) LiDAR sensors for strong 3D geometry priors that significantly improve the ray sampling locality, 
                 and 2) Conditional Adversarial Networks (cGANs) to recover image details since aggregating 
                 embeddings from imperfect LiDAR maps causes artifacts in the synthesized images. 
                 Our experiments show that while NeRF baselines produce either noisy or blurry results on Argoverse 2, 
                 the images synthesized by our system not only outperform baselines in image quality metrics under both 
                 clean and noisy conditions, but also obtain closer Detectron2 results to the ground truth images. Furthermore, 
                 to show the substantial applicability of our system, we demonstrate that our system can be used in 
                 data augmentation for training a pose regression network and multi-season view synthesis. 
                 Our dataset and code will be released.},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    month     = {October},
    year      = {2023},
    pages     = {17914-17923}
}

@inproceedings{Xu22iros,
    abbr = {IROS},
    pdf = {Xu22iros.pdf},
    author = {Ruoyang Xu and Wei Dong and Akash Sharma and Michael Kaess},
    title = {Learned Depth Estimation of {3D} Imaging Radar for Indoor Mapping},
    abstract = {3D imaging radar offers robust perception capability through
                visually demanding environments due to the unique penetrative and
                reflective properties of millimeter waves (mmWave). Current
                approaches for 3D perception with imaging radar require knowledge
                of environment geometry, accumulation of data from multiple
                frames for perception, or access to between-frame motion. Imaging
                radar presents an additional difficulty due to the complexity of
                its data representation. To address these issues, and make
                imaging radar easier to use for downstream robotics tasks, we
                propose a learning-based method that regresses radar measurements
                into cylindrical depth maps using LiDAR supervision. Due to the
                limitation of the regression formulation, directions where the
                radar beam could not reach will still generate a valid depth. To
                address this issue, our method additionally learns a 3D filter to
                remove those pixels. Experiments show that our system generates
                visually accurate depth estimation. Furthermore, we confirm the
                overall ability to generalize in the indoor scene using the
                estimated depth for probabilistic occupancy mapping with ground
                truth trajectory.},
    booktitle = {Proc. IEEE/RSJ Intl. Conf. on Intelligent Robots and Systems,
                 IROS},
    address = {Kyoto, Japan},
    month = oct,
    year = {2022},
    pages={13260-13267},
}

@inproceedings{Sharma21icra,
    abbr = {ICRA},
    selected = {true},
    pdf = {Sharma21icra.pdf},
    code = {https://github.com/rpl-cmu/object-slam},
    teaser = {object_slam.gif},
    author = {Akash Sharma and Wei Dong and Michael Kaess},
    title = {Compositional and Scalable Object {SLAM}},
    abstract = {We present a fast, scalable, and accurate Simultaneous
                Localization and Mapping (SLAM) system that represents indoor
                scenes as a graph of objects. Leveraging the observation that
                artificial environments are structured and occupied by
                recognizable objects, we show that a compositional and scalable
                object mapping formulation is amenable to a robust SLAM solution
                for drift-free large-scale indoor reconstruction. To achieve this
                , we propose a novel semantically assisted data association
                strategy that results in unambiguous persistent object landmarks
                and a 2.5D compositional rendering method that enables reliable
                frame-to-model RGB-D tracking. Consequently, we deliver an
                optimized online implementation that can run at near frame rate
                with a single graphics card, and provide a comprehensive
                evaluation against state-of-the-art baselines. An open-source
                implementation will be provided at
                https://github.com/rpl-cmu/object-slam.},
    booktitle = {Proc. IEEE Intl. Conf. on Robotics and Automation, ICRA},
    address = {Xi'an, China},
    month = may,
    year = {2021},
}

@thesis{Akash-Sharma-2021-127395,
    abbr = {M.S.},
    author = {Akash Sharma},
    pdf = {msr_thesis.pdf},
    title = {Incorporating Semantic Structure in SLAM},
    year = {2021},
    month = {May},
    school = {Carnegie Mellon University},
    address = {Pittsburgh, PA},
    number = {CMU-RI-TR-21-18},
    keywords = {Semantic SLAM, SLAM, Instance Segmentation, Dense reconstruction
                },
}